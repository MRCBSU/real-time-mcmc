#!/bin/bash

## Example SLURM script for BSU purcell jobs

## Section 1: SLURM Commands

## All SLURM commands must be placed at the start of the file
## Full documentation can be found here: https://slurm.schedmd.com/sbatch.html

## Enter a short name for the job, to be shown in SLURM output
#SBATCH -J RTM_20200625

## Enter the wall-clock time limit for for your jobs.
## If jobs reach this limit they are automatically killed.
#SBATCH --time=20:00:00

## For single-core jobs, this number should be '1'. 
## If your job has built-in parallelism, eg using OpenMP or 
## R's foreach() and doParallel(), increase this number as desired.
## The maximum value is 64.
#SBATCH --cpus-per-task=14

## Each task is allocated 16120M. 
## If this is insufficient, uncomment and edit this line.
## #SBATCH --mem=24G

## The system can send emails when your job starts and stops.
## Values include BEGIN, END, ALL, and TIME_LIMIT_80 and TIME_LIMIT_90 
## (reaching 80% or 90% of time limit.) Specify ARRAY_TASKS to receive
## a separate mail for each task. Multiple values can be given, separated by a comma.
#SBATCH --mail-type=ALL

## Array jobs:
## Can be an arbitrary list of comma-separated ranges, eg 1-5,10-15,20-25
## The %nn is optional, and restricts SLURM to running nn jobs simultaneously.
## #SBATCH --array=1-100%50
#SBATCH --array=1-4

#! sbatch directives end here (put any additional directives above this line)
#SBATCH --output=model_run_20200625_rw_sero%A_%a.out
#SBATCH --error=model_run_20200625_rw_sero%A_%a.err


##  - - - - - - - - - - - - - -

## Section 2: Modules

# All scripts should include the first three lines.

module purge                          # Removes all modules still loaded
module load default-login             # REQUIRED - loads the basic environment

#! Insert additional module load commands after this line if needed:

## - - - - - - - - - - -

## Section 3: Run your application

# Navigate to the correct working directory
workdir="$SLURM_SUBMIT_DIR/model_runs/20200717"
if [ $SLURM_ARRAY_TASK_ID == 1 ]
then
    workdir="${workdir}/base_6day_matrices_20200710_deaths/"
elif [ $SLURM_ARRAY_TASK_ID == 2 ]
then
    workdir="${workdir}/base_newContacts6day_matrices_20200717_deaths/"
elif [ $SLURM_ARRAY_TASK_ID == 3 ]
then
    workdir="${workdir}/base_newContacts_newSero6day_matrices_20200717_deaths/"
elif [ $SLURM_ARRAY_TASK_ID == 4 ]
then
    workdir="${workdir}/base_newContacts_newSero_altSens6day_matrices_20200717_deaths/"
fi
echo -e $workdir

# You can use any arbitrary set of Linux commands here

CMD="../../../rtm_optim"

# Or for example:
# CMD="Rscript myScript.R"


###############################################################
### You should not have to change anything below this line ####
###############################################################

cd $workdir
echo -e "Changed directory to `pwd`.\n"

JOBID=$SLURM_JOB_ID

echo -e "JobID: $JOBID\n======"
echo "Time: `date`"
if [ $SLURM_JOB_NUM_NODES -gt 1 ]; then
        echo "Running on nodes: $SLURM_JOB_NODELIST"
else
        echo "Running on node: `hostname`"
fi

echo "Current directory: `pwd`"
echo -e "\nNum tasks = $SLURM_NTASKS, Num nodes = $SLURM_JOB_NUM_NODES, OMP_NUM_THREADS = $OMP_NUM_THREADS"
echo -e "\nExecuting command:\n==================\n$CMD\n"

eval $CMD
